# ============================================================
# LiteLLM Proxy Configuration
# ============================================================
# Docs: https://docs.litellm.ai/docs/proxy/configs
#
# Environment variables are referenced with "os.environ/VAR_NAME".
# Set them in .env (Docker Compose) or Space Secrets (HF Spaces).
# ============================================================

# ----- Model Routing ------------------------------------------

model_list:
  # Primary: HuggingFace Inference Endpoint (or any HF-hosted model)
  - model_name: "default"
    litellm_params:
      model: "huggingface/Qwen/Qwen2.5-Coder-0.5B-Instruct"
      api_key: "os.environ/HF_TOKEN"
      # Uncomment and set if using a Dedicated Endpoint:
      # api_base: "https://your-endpoint.aws.endpoints.huggingface.cloud"

  # Fallback: OpenAI (optional, remove if not needed)
  # - model_name: "openai-fallback"
  #   litellm_params:
  #     model: "openai/gpt-4o-mini"
  #     api_key: "os.environ/OPENAI_API_KEY"

# ----- General Settings ---------------------------------------

general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"

  # Database is OPTIONAL. Without it, LiteLLM works but virtual keys
  # and spend tracking are disabled. Uncomment to enable:
  # database_url: "os.environ/DATABASE_URL"
  # enable_user_auth: true
  # ui_access_mode: "admin_only"

  # Global budget cap (USD). Set 0 to disable.
  max_budget: 10.0
  budget_duration: "30d"

# ----- Rate Limits --------------------------------------------

router_settings:
  routing_strategy: "simple-shuffle"    # load-balance across models
  num_retries: 2
  timeout: 30                           # seconds
  retry_after: 5                        # seconds between retries

# Per-model rate limits (applied to all virtual keys by default)
rate_limits:
  - model: "default"
    tpm: 10000      # tokens per minute
    rpm: 60          # requests per minute

# ----- LiteLLM Behaviour -------------------------------------

litellm_settings:
  telemetry: false
  drop_params: true                     # silently drop unsupported params
  set_verbose: false
  cache: true                           # enable in-memory response cache
  cache_params:
    type: "local"
    ttl: 600                            # cache TTL in seconds (10 min)
